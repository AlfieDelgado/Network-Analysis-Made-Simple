{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tables to Networks\n",
      "\n",
      "If you have relational data currently stored in a tabular format, then you've got data ready to go. \n",
      "\n",
      "Networks can be represented in a tabular form in two ways: As an adjacency list with edge attributes stored as columnar values, and as a node list with node attributes stored as columnar values.\n",
      "\n",
      "Here, we will get you set up and ready for the free-hacking session. To do this, we will learn how to load data from a CSV file into a Pandas DataFrame, and then iterate over the data to create the NetworkX network, which you can then use to perform the analyses that answer the exercise questions below. Along the way, we will also answer one network question, \"What set of stations are important?\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Loading Node Lists and Adjacency Lists\n",
      "\n",
      "Let's use the Divvy bike sharing data set as a starting point. The Divvy data set is comprised of the following data:\n",
      "\n",
      "- Stations and metadata (like a node list with attributes saved)\n",
      "- Trips and metadata (like an edge list with attributes saved)\n",
      "\n",
      "The `README.txt` file in the Divvy directory should help orient you around the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir('datasets/divvy_2013')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stations = pd.read_csv('Divvy_Stations_2013.csv', parse_dates=['online date'], index_col='id')\n",
      "stations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trips = pd.read_csv('Divvy_Trips_2013.csv', parse_dates=['starttime', 'stoptime'], index_col=['trip_id'])\n",
      "trips = trips.sort()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, we have our `stations` and `trips` data loaded into memory. How we construct the graph depends on the kind of questions we want to answer, so the definition of the \"unit of consideration\" is extremely important. In this case, the bike station is a reasonable \"unit of consideration\", so we will use the bike stations as the nodes.\n",
      "\n",
      "To start, let's initialize an directed graph `G`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G = nx.DiGraph()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, let's iterate over the `stations` DataFrame, and add in the node attributes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for r, d in stations.iterrows(): # call the pandas DataFrame row-by-row iterator\n",
      "    G.add_node(r, attr_dict=d.to_dict())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to answer the question of \"which stations are important\", we need to specify things a bit more. Perhaps a measure such as **betweenness centrality** or **degree centrality** may be appropriate here.\n",
      "\n",
      "The naive way would be to iterate over all the rows. Go ahead and try it at your own risk - it may take a long time :-). Alternatively, I would suggest doing a `pandas` `groupby`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Run the following code at your own risk :)\n",
      "# for r, d in trips.iterrows():\n",
      "#     start = d['from_station_id']\n",
      "#     end = d['to_station_id']\n",
      "#     if (start, end) not in G.edges():\n",
      "#         G.add_edge(start, end, count=1)\n",
      "#     else:\n",
      "#         G.edge[start][end]['count'] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (start, stop), d in trips.groupby(['from_station_id', 'to_station_id']):\n",
      "    G.add_edge(start, stop, count=len(d))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G.edges(data=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use the **betweenness centrality** metric."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "centralities = nx.betweenness_centrality(G, weight='count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(centralities.items(), key=lambda x:x[1], reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "plt.bar(centralities.keys(), centralities.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For comparison, let's also look at the **degree centrality** as a metric of station importance. Degree centrality does not consider the weighting, though, so it may not necessarily be the best metric."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "decentrality = nx.degree_centrality(G)\n",
      "plt.bar(decentrality.keys(), decentrality.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code above should have demonstrated to you the basic logic behind storing graph data in a human-readable format. For the richest data format, you can store a node list with attributes, and an edge list (a.k.a. adjacency list) with attributes. The n-by-n matrix format lets you succinctly capture the graph structure, but it is a bit harder to introduce rich metadata, which is usually the thing of interest."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Saving NetworkX Graph Files\n",
      "\n",
      "NetworkX's API offers many formats for storing graphs to disk. If you intend to work exclusively with NetworkX, then pickling the file to disk is probably the easiest way.\n",
      "\n",
      "To write to disk: \n",
      "\n",
      "    nx.write_gpickle(G, handle)\n",
      "\n",
      "To load from disk:\n",
      "    \n",
      "    G = nx.read_gpickle(handle)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
